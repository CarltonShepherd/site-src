Title: A Primer on Boosting (Pt. 1): An Overview
Date: 2019-10-10 08:46
Modified: 2019-10-10 08:46
Category: Python
Tags: machine-learning
Slug: adaboost
Authors: Carlton Shepherd
Summary: Short version for index and feeds
Status: draft

In this series of posts, we'll tour a family of machine learning algorithms known as *boosting*: what it is, how it works, its benefits, and its drawbacks. We'll also look at some of the most widely-used algorithms, such as Adaptive Boosting (AdaBoost) and Gradient Boosting Machines (GBMs), including XGBoost. 

Boosting has been used successfully to place highly many Kaggle competitions, including the [Higgs Boson](https://dbaumgartel.wordpress.com/2014/06/15/the-kaggle-higgs-challenge-beat-the-benchmarks-with-scikit-learn/), [product classification](https://www.kaggle.com/c/otto-group-product-classification-challenge/discussion/14335), and [IEEE-CIS fraud detection](https://www.kaggle.com/c/ieee-fraud-detection/discussion/111308#latest-645199) challenges. In general, it's found great utility using structured tabular data.

In this first post, an overview of boosting is given, alongside some of its high-level benefits and drawbacks; some fundamental knowledge of machine learning is assumed, such as decision trees and variance and bias notions.

## What is Boosting?
Boosting is a family of *meta-algorithms* that use an *ensemble* of *weak learners* to produce a *strong learner*. Let's break each of these terms down:

### Benefits


### Drawbacks
* 



## Refs.

1. Koblitz, ,
